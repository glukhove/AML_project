{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.set_printoptions(precision=3,threshold=np.inf, suppress=True)\n",
    "\n",
    "#local files\n",
    "from MLP import MLP\n",
    "from GraphCNN import GraphCNN\n",
    "\n",
    "torch.manual_seed(2)\n",
    "np.random.seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dgl.__version__ == '0.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../graphs_amino.pickle', 'rb') as gr:\n",
    "    all_A = pickle.load(gr)\n",
    "    \n",
    "with open('../lables_amino.pickle', 'rb') as la:\n",
    "    label = pickle.load(la)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([363, 632]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(label, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "label = list(label)\n",
    "max_label = int(max(label)) + 1\n",
    "print(max_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#undersampling\n",
    "class_1 = np.random.choice(np.where(np.array(label)==1)[0], len(np.where(np.array(label)==0)[0]), replace=False)\n",
    "inds = np.append(class_1, np.where(np.array(label)==0)[0])\n",
    "inds = np.random.permutation(inds)\n",
    "all_A = list(np.array(all_A)[inds])\n",
    "label = list(np.array(label)[inds])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Isomorphism Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_graphs, optimizer, epoch):\n",
    "    model.train()\n",
    "\n",
    "    total_iters = iters_per_epoch\n",
    "#     pbar = tqdm(range(total_iters), unit='batch')\n",
    "    pbar = range(total_iters)\n",
    "\n",
    "    loss_accum = 0\n",
    "    n_iter = 0\n",
    "    for pos in pbar:\n",
    "        selected_idx = np.random.permutation(len(train_graphs))[:batch_size]\n",
    "                \n",
    "        batch_graph = [train_graphs[idx][0] for idx in selected_idx]\n",
    "        labels = torch.FloatTensor([train_graphs[idx][1] for idx in selected_idx])\n",
    "        \n",
    "        output = model(batch_graph)\n",
    "                \n",
    "        loss = criterion(output, labels.view_as(output))\n",
    "\n",
    "        #backprop\n",
    "        if optimizer is not None:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()         \n",
    "            optimizer.step()\n",
    "        \n",
    "\n",
    "        loss = loss.detach().numpy()\n",
    "        loss_accum += loss\n",
    "        n_iter += 1\n",
    "\n",
    "        #report\n",
    "#         pbar.set_description('epoch: %d' % (epoch))\n",
    "\n",
    "    average_loss = loss_accum/n_iter\n",
    "    print(f\"epoch: {epoch}, \\t loss training: {average_loss}\", end='\\t')\n",
    "    \n",
    "    return average_loss\n",
    "\n",
    "###pass data to model with minibatch during testing to avoid memory overflow (does not perform backpropagation)\n",
    "def pass_data_iteratively(model, graphs, minibatch_size = 64):\n",
    "    model.eval()\n",
    "    output = []\n",
    "    idx = np.arange(len(graphs))\n",
    "    for i in range(0, len(graphs), minibatch_size):\n",
    "        sampled_idx = idx[i:i+minibatch_size]\n",
    "        if len(sampled_idx) == 0:\n",
    "            continue\n",
    "        output.append(model([graphs[j] for j in sampled_idx]).detach())\n",
    "    return torch.cat(output, 0)\n",
    "\n",
    "def test(model, train_graphs, test_graphs, epoch):\n",
    "    model.eval()\n",
    "    \n",
    "    train_graphs = train_graphs[:int(len(test_graphs)/batch_size)*batch_size]\n",
    "    batch_graph_train = [train_graph_[0] for train_graph_ in train_graphs]\n",
    "    labels = torch.FloatTensor([train_graph_[1] for train_graph_ in train_graphs])\n",
    "    output = pass_data_iteratively(model, batch_graph_train)\n",
    "    output = torch.round(torch.sigmoid(output))\n",
    "    correct = output.eq(labels.view_as(output)).sum().item()\n",
    "    acc_train = correct / float(len(train_graphs))\n",
    "    print(\"accuracy train: %f\" % (acc_train), end='\\t')\n",
    "    \n",
    "    \n",
    "    #############################################################\n",
    "    test_graphs = test_graphs[:int(len(test_graphs)/batch_size)*batch_size]\n",
    "    batch_graph = [test_graph_[0] for test_graph_ in test_graphs]\n",
    "    labels = torch.FloatTensor([test_graph_[1] for test_graph_ in test_graphs])\n",
    "        \n",
    "    #### we will not use pass_data_iteratively for now as we do not have a lot of data\n",
    "    output = pass_data_iteratively(model, batch_graph, batch_size)\n",
    "#     output = model(batch_graph)\n",
    "    output = torch.round(torch.sigmoid(output))\n",
    "\n",
    "    correct = output.eq(labels.view_as(output)).sum().item()\n",
    "    acc_test = correct / float(len(test_graphs))\n",
    "    \n",
    "    if print_mode == 0:\n",
    "        print(output.view(-1))\n",
    "\n",
    "    print(f\"accuracy test: {acc_test}\") #accuracy train: {acc_train};\n",
    "\n",
    "    return acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = []\n",
    "for z in zip(all_A, label):\n",
    "    arr.append(z)\n",
    "\n",
    "    \n",
    "# trainset = arr\n",
    "trainset, testset = train_test_split(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_mode=-1\n",
    "### if 0 - print output; if - 1 print every step matrices; if - 2 print logits from the net\n",
    "#'which gpu to use if any (default: 0)'\n",
    "batch_size=64\n",
    "# 'input batch size for training (default: 32)'\n",
    "iters_per_epoch=int(len(trainset)/batch_size)\n",
    "# 'number of iterations per each epoch'\n",
    "epochs=100\n",
    "#'number of epochs to train (default: 40)'\n",
    "lr=0.1\n",
    "#'learning rate (default: 0.01)'\n",
    "num_mlp_layers=2\n",
    "num_mlp_pooling_layers = 2\n",
    "mpl_layers_pred = 2\n",
    "#'number of layers for MLP EXCLUDING the input one (default: 2). 1 means linear model.'\n",
    "hidden_dim=8\n",
    "#'number of hidden units (default: 64)'\n",
    "final_dropout=0.3\n",
    "#'final layer dropout (default: 0.5)'\n",
    "update_layers=4\n",
    "#number of intermidiate GCN layers\n",
    "mlp_pred_factor = 1\n",
    "# number which hidden size will be multiplied for predictions\n",
    "\n",
    "\n",
    "gamma = 0.1\n",
    "#parameter of reducing lr\n",
    "step_size = 5\n",
    "# number of epochs atfer every n epochs lr witll be multiplied by gamma\n",
    "\n",
    "\n",
    "num_classes = 1\n",
    "\n",
    "poolings = [9, 1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, \t loss training: 0.7020971328020096\taccuracy train: 0.476562\taccuracy test: 0.484375\n",
      "epoch: 2, \t loss training: 0.6872038170695305\taccuracy train: 0.476562\taccuracy test: 0.484375\n",
      "epoch: 3, \t loss training: 0.6842877045273781\taccuracy train: 0.492188\taccuracy test: 0.4921875\n",
      "epoch: 4, \t loss training: 0.6854801923036575\taccuracy train: 0.546875\taccuracy test: 0.578125\n",
      "epoch: 5, \t loss training: 0.6873034462332726\taccuracy train: 0.523438\taccuracy test: 0.53125\n",
      "epoch: 6, \t loss training: 0.685721643269062\taccuracy train: 0.570312\taccuracy test: 0.6015625\n",
      "epoch: 7, \t loss training: 0.6785129532217979\taccuracy train: 0.625000\taccuracy test: 0.65625\n",
      "epoch: 8, \t loss training: 0.6549861282110214\taccuracy train: 0.585938\taccuracy test: 0.6484375\n",
      "epoch: 9, \t loss training: 0.6751667186617851\taccuracy train: 0.562500\taccuracy test: 0.65625\n",
      "epoch: 10, \t loss training: 0.6701731234788895\taccuracy train: 0.578125\taccuracy test: 0.6171875\n",
      "epoch: 11, \t loss training: 0.6770280301570892\taccuracy train: 0.578125\taccuracy test: 0.6171875\n",
      "epoch: 12, \t loss training: 0.6671407520771027\taccuracy train: 0.570312\taccuracy test: 0.625\n",
      "epoch: 13, \t loss training: 0.6606872007250786\taccuracy train: 0.578125\taccuracy test: 0.6328125\n",
      "epoch: 14, \t loss training: 0.6621270999312401\taccuracy train: 0.570312\taccuracy test: 0.6328125\n",
      "epoch: 15, \t loss training: 0.6783313304185867\taccuracy train: 0.570312\taccuracy test: 0.6171875\n",
      "epoch: 16, \t loss training: 0.671741358935833\taccuracy train: 0.570312\taccuracy test: 0.6328125\n",
      "epoch: 17, \t loss training: 0.6752739697694778\taccuracy train: 0.578125\taccuracy test: 0.6328125\n",
      "epoch: 18, \t loss training: 0.6561957374215126\taccuracy train: 0.578125\taccuracy test: 0.6171875\n",
      "epoch: 19, \t loss training: 0.6697079464793205\taccuracy train: 0.562500\taccuracy test: 0.625\n",
      "epoch: 20, \t loss training: 0.6697104722261429\taccuracy train: 0.570312\taccuracy test: 0.625\n",
      "epoch: 21, \t loss training: 0.6814036965370178\taccuracy train: 0.578125\taccuracy test: 0.6328125\n",
      "epoch: 22, \t loss training: 0.6564987599849701\taccuracy train: 0.578125\taccuracy test: 0.625\n",
      "epoch: 23, \t loss training: 0.6867260858416557\taccuracy train: 0.562500\taccuracy test: 0.625\n",
      "epoch: 24, \t loss training: 0.6720117256045341\taccuracy train: 0.578125\taccuracy test: 0.6328125\n",
      "epoch: 25, \t loss training: 0.6885222643613815\taccuracy train: 0.578125\taccuracy test: 0.6328125\n",
      "epoch: 26, \t loss training: 0.6736559867858887\taccuracy train: 0.578125\taccuracy test: 0.625\n",
      "epoch: 27, \t loss training: 0.6886203438043594\taccuracy train: 0.578125\taccuracy test: 0.640625\n",
      "epoch: 28, \t loss training: 0.6717958971858025\taccuracy train: 0.578125\taccuracy test: 0.6328125\n",
      "epoch: 29, \t loss training: 0.6925255507230759\taccuracy train: 0.562500\taccuracy test: 0.625\n",
      "epoch: 30, \t loss training: 0.666368305683136\taccuracy train: 0.562500\taccuracy test: 0.625\n",
      "epoch: 31, \t loss training: 0.654228612780571\taccuracy train: 0.578125\taccuracy test: 0.6328125\n",
      "epoch: 32, \t loss training: 0.6817536056041718\taccuracy train: 0.578125\taccuracy test: 0.625\n",
      "epoch: 33, \t loss training: 0.6820326372981071\taccuracy train: 0.562500\taccuracy test: 0.625\n",
      "epoch: 34, \t loss training: 0.6690223440527916\taccuracy train: 0.562500\taccuracy test: 0.640625\n",
      "epoch: 35, \t loss training: 0.664463721215725\taccuracy train: 0.562500\taccuracy test: 0.6328125\n",
      "epoch: 36, \t loss training: 0.6737142875790596\taccuracy train: 0.562500\taccuracy test: 0.6328125\n",
      "epoch: 37, \t loss training: 0.6668021380901337\taccuracy train: 0.562500\taccuracy test: 0.6328125\n",
      "epoch: 38, \t loss training: 0.6685577407479286\taccuracy train: 0.578125\taccuracy test: 0.6328125\n",
      "epoch: 39, \t loss training: 0.681005448102951\taccuracy train: 0.578125\taccuracy test: 0.6328125\n",
      "epoch: 40, \t loss training: 0.6720864772796631\taccuracy train: 0.578125\taccuracy test: 0.6328125\n",
      "epoch: 41, \t loss training: 0.6797364801168442\taccuracy train: 0.554688\taccuracy test: 0.625\n",
      "epoch: 42, \t loss training: 0.6976309642195702\taccuracy train: 0.562500\taccuracy test: 0.625\n",
      "epoch: 43, \t loss training: 0.681086353957653\taccuracy train: 0.562500\taccuracy test: 0.6328125\n",
      "epoch: 44, \t loss training: 0.6660301014780998\taccuracy train: 0.562500\taccuracy test: 0.625\n",
      "epoch: 45, \t loss training: 0.6599500775337219\taccuracy train: 0.578125\taccuracy test: 0.625\n",
      "epoch: 46, \t loss training: 0.6767362877726555\taccuracy train: 0.578125\taccuracy test: 0.6328125\n",
      "epoch: 47, \t loss training: 0.6705841049551964\taccuracy train: 0.578125\taccuracy test: 0.6328125\n",
      "epoch: 48, \t loss training: 0.6827153712511063\taccuracy train: 0.562500\taccuracy test: 0.625\n",
      "epoch: 49, \t loss training: 0.678220808506012\taccuracy train: 0.578125\taccuracy test: 0.640625\n",
      "epoch: 50, \t loss training: 0.661112941801548\taccuracy train: 0.578125\taccuracy test: 0.625\n",
      "epoch: 51, \t loss training: 0.6946937367320061\taccuracy train: 0.578125\taccuracy test: 0.6328125\n",
      "epoch: 52, \t loss training: 0.6659985408186913\taccuracy train: 0.562500\taccuracy test: 0.6328125\n",
      "epoch: 53, \t loss training: 0.6620606929063797\taccuracy train: 0.578125\taccuracy test: 0.6328125\n",
      "epoch: 54, \t loss training: 0.6774475574493408\taccuracy train: 0.562500\taccuracy test: 0.625\n",
      "epoch: 55, \t loss training: 0.6761692315340042\taccuracy train: 0.578125\taccuracy test: 0.6328125\n",
      "epoch: 56, \t loss training: 0.661731943488121\taccuracy train: 0.578125\taccuracy test: 0.6328125\n",
      "epoch: 57, \t loss training: 0.6686995774507523\taccuracy train: 0.578125\taccuracy test: 0.6328125\n",
      "epoch: 58, \t loss training: 0.6585500314831734\taccuracy train: 0.562500\taccuracy test: 0.6328125\n",
      "epoch: 59, \t loss training: 0.6771079078316689\taccuracy train: 0.578125\taccuracy test: 0.6328125\n",
      "epoch: 60, \t loss training: 0.6883438676595688\taccuracy train: 0.562500\taccuracy test: 0.625\n",
      "epoch: 61, \t loss training: 0.6789549291133881\taccuracy train: 0.578125\taccuracy test: 0.6328125\n",
      "epoch: 62, \t loss training: 0.6813120543956757\taccuracy train: 0.554688\taccuracy test: 0.625\n",
      "epoch: 63, \t loss training: 0.6764467731118202\taccuracy train: 0.562500\taccuracy test: 0.6328125\n",
      "epoch: 64, \t loss training: 0.6604923009872437\taccuracy train: 0.562500\taccuracy test: 0.625\n",
      "epoch: 65, \t loss training: 0.6816897392272949\taccuracy train: 0.554688\taccuracy test: 0.625\n",
      "epoch: 66, \t loss training: 0.6739422604441643\taccuracy train: 0.578125\taccuracy test: 0.625\n",
      "epoch: 67, \t loss training: 0.6745557487010956\taccuracy train: 0.578125\taccuracy test: 0.640625\n",
      "epoch: 68, \t loss training: 0.655242033302784\taccuracy train: 0.578125\taccuracy test: 0.640625\n",
      "epoch: 69, \t loss training: 0.687593087553978\taccuracy train: 0.578125\taccuracy test: 0.6328125\n",
      "epoch: 70, \t loss training: 0.6703480258584023\taccuracy train: 0.578125\taccuracy test: 0.625\n",
      "epoch: 71, \t loss training: 0.672046035528183\taccuracy train: 0.562500\taccuracy test: 0.625\n",
      "epoch: 72, \t loss training: 0.6719077154994011\taccuracy train: 0.578125\taccuracy test: 0.6328125\n",
      "epoch: 73, \t loss training: 0.6786364167928696\taccuracy train: 0.578125\taccuracy test: 0.640625\n",
      "epoch: 74, \t loss training: 0.665810227394104\taccuracy train: 0.578125\taccuracy test: 0.625\n",
      "epoch: 75, \t loss training: 0.6850984171032906\taccuracy train: 0.562500\taccuracy test: 0.6328125\n",
      "epoch: 76, \t loss training: 0.6888435184955597\taccuracy train: 0.562500\taccuracy test: 0.625\n",
      "epoch: 77, \t loss training: 0.6709276735782623\taccuracy train: 0.562500\taccuracy test: 0.625\n",
      "epoch: 78, \t loss training: 0.6719571352005005\taccuracy train: 0.562500\taccuracy test: 0.625\n",
      "epoch: 79, \t loss training: 0.6750201061367989\taccuracy train: 0.578125\taccuracy test: 0.6328125\n",
      "epoch: 80, \t loss training: 0.6813675537705421\taccuracy train: 0.554688\taccuracy test: 0.625\n",
      "epoch: 81, \t loss training: 0.6481273397803307\taccuracy train: 0.562500\taccuracy test: 0.625\n",
      "epoch: 82, \t loss training: 0.6751975268125534\taccuracy train: 0.562500\taccuracy test: 0.625\n",
      "epoch: 83, \t loss training: 0.6533076241612434\taccuracy train: 0.554688\taccuracy test: 0.625\n",
      "epoch: 84, \t loss training: 0.6695296987891197\taccuracy train: 0.578125\taccuracy test: 0.6328125\n",
      "epoch: 85, \t loss training: 0.6807746514678001\taccuracy train: 0.578125\taccuracy test: 0.625\n",
      "epoch: 86, \t loss training: 0.6707764193415642\taccuracy train: 0.578125\taccuracy test: 0.6328125\n",
      "epoch: 87, \t loss training: 0.6770667061209679\taccuracy train: 0.578125\taccuracy test: 0.640625\n",
      "epoch: 88, \t loss training: 0.6711377426981926\taccuracy train: 0.578125\taccuracy test: 0.625\n",
      "epoch: 89, \t loss training: 0.6639086604118347\taccuracy train: 0.578125\taccuracy test: 0.6328125\n",
      "epoch: 90, \t loss training: 0.663728691637516\taccuracy train: 0.578125\taccuracy test: 0.625\n",
      "epoch: 91, \t loss training: 0.6824895665049553\taccuracy train: 0.570312\taccuracy test: 0.6328125\n",
      "epoch: 92, \t loss training: 0.6898069903254509\taccuracy train: 0.578125\taccuracy test: 0.640625\n",
      "epoch: 93, \t loss training: 0.6814106702804565\taccuracy train: 0.562500\taccuracy test: 0.625\n",
      "epoch: 94, \t loss training: 0.6697442382574081\taccuracy train: 0.578125\taccuracy test: 0.6328125\n",
      "epoch: 95, \t loss training: 0.6813483387231827\taccuracy train: 0.578125\taccuracy test: 0.6328125\n",
      "epoch: 96, \t loss training: 0.6859390363097191\taccuracy train: 0.578125\taccuracy test: 0.625\n",
      "epoch: 97, \t loss training: 0.6776935160160065\taccuracy train: 0.578125\taccuracy test: 0.6328125\n",
      "epoch: 98, \t loss training: 0.6929629221558571\taccuracy train: 0.578125\taccuracy test: 0.6328125\n",
      "epoch: 99, \t loss training: 0.6665737107396126\taccuracy train: 0.578125\taccuracy test: 0.6328125\n",
      "epoch: 100, \t loss training: 0.6673993617296219\taccuracy train: 0.562500\taccuracy test: 0.625\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "losses = []\n",
    "acc = []\n",
    "\n",
    "\n",
    "model = GraphCNN(batch_size, num_mlp_layers, num_mlp_pooling_layers, trainset[0][0].ndata['h'].shape[1], hidden_dim, num_classes, final_dropout, poolings, update_layers, mpl_layers_pred, mlp_pred_factor, print_mode)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    scheduler.step()   \n",
    "\n",
    "    avg_loss = train(model, trainset, optimizer, epoch)\n",
    "    losses.append(avg_loss)\n",
    "    if epoch % 1 == 0:\n",
    "        acc_test = test(model, trainset, testset, epoch)\n",
    "        acc.append(acc_test)\n",
    "    \n",
    "    \n",
    "    if print_mode == 1: \n",
    "        print(model.edge_features)\n",
    "        print('_____________________')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
