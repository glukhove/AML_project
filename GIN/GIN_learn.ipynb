{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#local files\n",
    "from MLP import MLP\n",
    "from GIN_CNN import GIN_CNN\n",
    "\n",
    "torch.manual_seed(2)\n",
    "np.random.seed(2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dgl.__version__ == '0.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../graphs_amino.pickle', 'rb') as gr:\n",
    "    all_A = pickle.load(gr)\n",
    "    \n",
    "with open('../lables_amino.pickle', 'rb') as la:\n",
    "    label = pickle.load(la)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([363, 632]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(label, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "label = list(label)\n",
    "max_label = int(max(label)) + 1\n",
    "print(max_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#undersampling\n",
    "class_1 = np.random.choice(np.where(np.array(label)==1)[0], len(np.where(np.array(label)==0)[0]), replace=False)\n",
    "inds = np.append(class_1, np.where(np.array(label)==0)[0])\n",
    "inds = np.random.permutation(inds)\n",
    "all_A = list(np.array(all_A)[inds])\n",
    "label = list(np.array(label)[inds])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Isomorphism Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_graphs, optimizer, epoch):\n",
    "    model.train()\n",
    "\n",
    "    total_iters = iters_per_epoch\n",
    "#     pbar = tqdm(range(total_iters), unit='batch')\n",
    "    pbar = range(total_iters)\n",
    "\n",
    "    loss_accum = 0\n",
    "    n_iter = 0\n",
    "    for pos in pbar:\n",
    "        selected_idx = np.random.permutation(len(train_graphs))[:batch_size]\n",
    "                \n",
    "        batch_graph = [train_graphs[idx][0] for idx in selected_idx]\n",
    "        labels = torch.FloatTensor([train_graphs[idx][1] for idx in selected_idx])\n",
    "        \n",
    "        output = model(batch_graph)\n",
    "        \n",
    "        loss = criterion(output, labels.view_as(output))\n",
    "\n",
    "        #backprop\n",
    "        if optimizer is not None:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()         \n",
    "            optimizer.step()\n",
    "        \n",
    "\n",
    "        loss = loss.detach().numpy()\n",
    "        loss_accum += loss\n",
    "        n_iter += 1\n",
    "\n",
    "        #report\n",
    "#         pbar.set_description('epoch: %d' % (epoch))\n",
    "\n",
    "    average_loss = loss_accum/n_iter\n",
    "    print(f\"epoch: {epoch}, \\t loss training: {average_loss}\", end='\\t')\n",
    "    \n",
    "    return average_loss\n",
    "\n",
    "###pass data to model with minibatch during testing to avoid memory overflow (does not perform backpropagation)\n",
    "def pass_data_iteratively(model, graphs, minibatch_size = 64):\n",
    "    model.eval()\n",
    "    output = []\n",
    "    idx = np.arange(len(graphs))\n",
    "    for i in range(0, len(graphs), minibatch_size):\n",
    "        sampled_idx = idx[i:i+minibatch_size]\n",
    "        if len(sampled_idx) == 0:\n",
    "            continue\n",
    "        output.append(model([graphs[j] for j in sampled_idx]).detach())\n",
    "    return torch.cat(output, 0)\n",
    "\n",
    "def test(model, train_graphs, test_graphs, epoch):\n",
    "    model.eval()\n",
    "    \n",
    "    \n",
    "    batch_graph_train = [train_graph_[0] for train_graph_ in train_graphs]\n",
    "    labels = torch.FloatTensor([train_graph_[1] for train_graph_ in train_graphs])\n",
    "    output = pass_data_iteratively(model, batch_graph_train)\n",
    "    output = torch.round(torch.sigmoid(output))\n",
    "    correct = output.eq(labels.view_as(output)).sum().cpu().item()\n",
    "    acc_train = correct / float(len(train_graphs))\n",
    "    print(\"accuracy train: %f\" % (acc_train), end='\\t')\n",
    "    \n",
    "    \n",
    "    #############################################################\n",
    "\n",
    "    batch_graph = [test_graph_[0] for test_graph_ in test_graphs]\n",
    "    labels = torch.FloatTensor([test_graph_[1] for test_graph_ in test_graphs])\n",
    "        \n",
    "    #### we will not use pass_data_iteratively for now as we do not have a lot of data\n",
    "    output = model(batch_graph)\n",
    "    output = torch.round(torch.sigmoid(output))\n",
    "\n",
    "    correct = output.eq(labels.view_as(output)).sum().item()\n",
    "    acc_test = correct / float(len(test_graphs))\n",
    "\n",
    "    print(f\"accuracy test: {acc_test}\") #accuracy train: {acc_train};\n",
    "\n",
    "    return acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = []\n",
    "for z in zip(all_A, label):\n",
    "    arr.append(z)\n",
    "\n",
    "trainset, testset = train_test_split(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "# 'input batch size for training (default: 32)'\n",
    "iters_per_epoch=int(len(trainset)/batch_size)\n",
    "# 'number of iterations per each epoch (default: 50)'\n",
    "epochs=30\n",
    "#'number of epochs to train (default: 350)'\n",
    "lr=0.01\n",
    "#'learning rate (default: 0.01)'\n",
    "num_layers = 2\n",
    "#'number of layers INCLUDING the input one (default: 5)'\n",
    "num_mlp_layers=2\n",
    "#'number of layers for MLP EXCLUDING the input one (default: 2). 1 means linear model.'\n",
    "hidden_dim=8\n",
    "#'number of hidden units (default: 64)'\n",
    "final_dropout=0.1\n",
    "#'final layer dropout (default: 0.5)'\n",
    "\n",
    "print_mode=0\n",
    "\n",
    "num_classes = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, \t loss training: 1.1846226304769516\taccuracy train: 0.494485\taccuracy test: 0.4835164835164835\n",
      "epoch: 2, \t loss training: 0.8808127045631409\taccuracy train: 0.477941\taccuracy test: 0.5054945054945055\n",
      "epoch: 3, \t loss training: 0.7640379443764687\taccuracy train: 0.472426\taccuracy test: 0.489010989010989\n",
      "epoch: 4, \t loss training: 0.7723512127995491\taccuracy train: 0.511029\taccuracy test: 0.521978021978022\n",
      "epoch: 5, \t loss training: 0.733861654996872\taccuracy train: 0.512868\taccuracy test: 0.5274725274725275\n",
      "epoch: 6, \t loss training: 0.690263070166111\taccuracy train: 0.560662\taccuracy test: 0.5769230769230769\n",
      "epoch: 7, \t loss training: 0.6783007308840752\taccuracy train: 0.602941\taccuracy test: 0.5274725274725275\n",
      "epoch: 8, \t loss training: 0.6744397431612015\taccuracy train: 0.586397\taccuracy test: 0.5604395604395604\n",
      "epoch: 9, \t loss training: 0.6961414739489555\taccuracy train: 0.514706\taccuracy test: 0.5054945054945055\n",
      "epoch: 10, \t loss training: 0.6850193440914154\taccuracy train: 0.593750\taccuracy test: 0.532967032967033\n",
      "epoch: 11, \t loss training: 0.6860853806138039\taccuracy train: 0.599265\taccuracy test: 0.5659340659340659\n",
      "epoch: 12, \t loss training: 0.6518892794847488\taccuracy train: 0.601103\taccuracy test: 0.5769230769230769\n",
      "epoch: 13, \t loss training: 0.6685903295874596\taccuracy train: 0.606618\taccuracy test: 0.5604395604395604\n",
      "epoch: 14, \t loss training: 0.6500137448310852\taccuracy train: 0.623162\taccuracy test: 0.6098901098901099\n",
      "epoch: 15, \t loss training: 0.6669454202055931\taccuracy train: 0.621324\taccuracy test: 0.6098901098901099\n",
      "epoch: 16, \t loss training: 0.6478380933403969\taccuracy train: 0.623162\taccuracy test: 0.5934065934065934\n",
      "epoch: 17, \t loss training: 0.6651807874441147\taccuracy train: 0.634191\taccuracy test: 0.5934065934065934\n",
      "epoch: 18, \t loss training: 0.6752449348568916\taccuracy train: 0.615809\taccuracy test: 0.5989010989010989\n",
      "epoch: 19, \t loss training: 0.6456762850284576\taccuracy train: 0.617647\taccuracy test: 0.5769230769230769\n",
      "epoch: 20, \t loss training: 0.6589933931827545\taccuracy train: 0.615809\taccuracy test: 0.5879120879120879\n",
      "epoch: 21, \t loss training: 0.6531328707933426\taccuracy train: 0.621324\taccuracy test: 0.5879120879120879\n",
      "epoch: 22, \t loss training: 0.6516025215387344\taccuracy train: 0.623162\taccuracy test: 0.5824175824175825\n",
      "epoch: 23, \t loss training: 0.6568795666098595\taccuracy train: 0.619485\taccuracy test: 0.5824175824175825\n",
      "epoch: 24, \t loss training: 0.6459059119224548\taccuracy train: 0.626838\taccuracy test: 0.5714285714285714\n",
      "epoch: 25, \t loss training: 0.649835079908371\taccuracy train: 0.630515\taccuracy test: 0.6098901098901099\n",
      "epoch: 26, \t loss training: 0.6514755412936211\taccuracy train: 0.634191\taccuracy test: 0.6098901098901099\n",
      "epoch: 27, \t loss training: 0.6626026257872581\taccuracy train: 0.628676\taccuracy test: 0.6043956043956044\n",
      "epoch: 28, \t loss training: 0.6649606227874756\taccuracy train: 0.628676\taccuracy test: 0.5989010989010989\n",
      "epoch: 29, \t loss training: 0.6481029912829399\taccuracy train: 0.623162\taccuracy test: 0.5824175824175825\n",
      "epoch: 30, \t loss training: 0.6506141945719719\taccuracy train: 0.623162\taccuracy test: 0.6098901098901099\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "model = GIN_CNN(num_layers, num_mlp_layers, trainset[0][0].ndata['h'].shape[1], hidden_dim, num_classes, final_dropout)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.2)\n",
    "\n",
    "losses=[]\n",
    "acc = []\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    scheduler.step()\n",
    "\n",
    "    avg_loss = train(model, trainset, optimizer, epoch)\n",
    "    losses.append(avg_loss)\n",
    "    acc_test = test(model, trainset, testset, epoch)\n",
    "    acc.append(acc_test)\n",
    "\n",
    "\n",
    "    if print_mode==1:\n",
    "        print(model.edge_features)\n",
    "        print('_____________________')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
